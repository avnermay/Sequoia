Following environment is needed:
transformer 4.37.2
trl 0.7.11
accelerate 0.30.0
torch 2.2.2

Steps:
1. Preprocess training data using src/preprocess_sharegpt.py
2. Generate a 2 or 4 layer checkpoint by pruning layers from target model using src/init_ckpt.py
3. Launch train_launch_scripts/llama3_8b_eagle.sh, but fixing paths.